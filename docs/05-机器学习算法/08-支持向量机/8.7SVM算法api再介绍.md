# 8.7 SVM算法api再介绍

## 学习目标

- 知道SVM算法api中的SVC、NuSVC、LinearSVC

------



## 1 SVM算法api综述

- SVM方法既可以用于分类（二/多分类），也可用于回归和异常值检测。


- SVM具有良好的鲁棒性，对未知数据拥有很强的泛化能力，**特别是在数据量较少的情况下**，相较其他传统机器学习算法具有更优的性能。



---

使用SVM作为模型时，通常采用如下流程：

1. 对样本数据进行归一化
2. 应用核函数对样本进行映射**（最常采用和核函数是RBF和Linear，在样本线性可分时，Linear效果要比RBF好）**
3. 用cross-validation和grid-search对超参数进行优选
4. 用最优参数训练得到模型
5. 测试

---

sklearn中支持向量分类主要有三种方法：SVC、NuSVC、LinearSVC，扩展为三个支持向量回归方法：SVR、NuSVR、LinearSVR。

- SVC和NuSVC方法基本一致，唯一区别就是损失函数的度量方式不同
    - NuSVC中的nu参数和SVC中的C参数；
- LinearSVC是实现线性核函数的支持向量分类，没有kernel参数。



## 2 SVC

```python
class sklearn.svm.SVC(C=1.0, kernel='rbf', degree=3,coef0=0.0,random_state=None)　
```

- **C:** 惩罚系数，用来控制损失函数的惩罚系数，类似于线性回归中的正则化系数。
    - C越大，相当于惩罚松弛变量，希望松弛变量接近0，即**对误分类的惩罚增大**，趋向于对训练集全分对的情况，这样会出现训练集测试时准确率很高，但泛化能力弱，容易导致过拟合。 
    - C值小，**对误分类的惩罚减小**，容错能力增强，泛化能力较强，但也可能欠拟合。
- **kernel:** 算法中采用的核函数类型，核函数是用来将非线性问题转化为线性问题的一种方法。
    - 参数选择有RBF, Linear, Poly, Sigmoid或者自定义一个核函数。
        -  默认的是"RBF"，即径向基核，也就是高斯核函数；
        -  而Linear指的是线性核函数，
        -  Poly指的是多项式核，
        -  Sigmoid指的是双曲正切函数tanh核；。
- **degree:** 
    - 当指定kernel为'poly'时，表示选择的多项式的最高次数，默认为三次多项式；
    - 若指定kernel不是'poly'，则忽略，即该参数只对'poly'有用。
        - 多项式核函数是将低维的输入空间映射到高维的特征空间。
- **coef0:** 核函数常数值(y=kx+b中的b值)，
    - 只有‘poly’和‘sigmoid’核函数有，默认值是0。



## 3 NuSVC　　　　　　　　　　　　　　　　

```python
class sklearn.svm.NuSVC(nu=0.5)
```

- **nu：** 训练误差部分的上限和支持向量部分的下限，取值在（0，1）之间，默认是0.5



## 4 LinearSVC　　　　

```python
class sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', dual=True, C=1.0)
```

- penalty:正则化参数，
    - L1和L2两种参数可选，仅LinearSVC有。
- loss:损失函数，
    - 有hinge和squared_hinge两种可选，前者又称L1损失，后者称为L2损失，默认是squared_hinge，
    - 其中hinge是SVM的标准损失，squared_hinge是hinge的平方
- dual:是否转化为对偶问题求解，默认是True。
- C:惩罚系数，
    - 用来控制损失函数的惩罚系数，类似于线性回归中的正则化系数。



------

## 3 小结

- SVM的核方法
    - 将原始输入空间映射到新的特征空间，从而，使得原本线性不可分的样本可能在核空间可分。
- SVM算法api
    - sklearn.svm.SVC
    - sklearn.svm.NuSVC
    - sklearn.svm.LinearSVC