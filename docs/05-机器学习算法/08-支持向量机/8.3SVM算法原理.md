# 8.3 SVM算法原理

## 学习目标

- 知道SVM中线性可分支持向量机
- 知道SVM中目标函数的推导过程
- 了解朗格朗日乘子法、对偶问题
- 知道SVM中目标函数的求解过程

------



## 1 定义输入数据

假设给定一个特征空间上的训练集为：

$$T=\{(x_1, y_1),(x_2,y_2)...,(x_N,y_N)\}$$

$$x_i \in R^n, y_i \in \{+1, -1\}, i=1,2,...,N.$$



其中，(x<sub>i</sub>,y<sub>i</sub>)称为样本点。

- x<sub>i</sub>为第i个实例（样本），

- y<sub>i</sub>为的x<sub>i</sub>标记： 
    - 当y<sub>i</sub>=1时，为x<sub>i</sub>正例 
    - 当y<sub>i</sub>=-1时，为x<sub>i</sub>负例 

> 至于为什么正负用（-1，1）表示呢？
>
> 其实这里没有太多原理，就是一个标记，你也可以用(2，-3)来标记。只是为了方便，$$y_i/y_j=y_i*y_j$$的过程中刚好可以相等，便于之后的计算。）



## 2 线性可分支持向量机

给定了上面提出的线性可分训练数据集，通过间隔最大化得到分离超平面为 :$$y(x)=w^T\Phi(x)+b$$

相应的分类决策函数为： $$f(x)=sign(w^T\Phi(x)+b)$$

以上决策函数就称为线性可分支持向量机。

这里解释一下<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfiyd4qndej30340283yb.jpg" alt="image-20200606224643722" style="zoom:33%;" />这个东东。 

这是某个确定的特征空间转换函数，它的作用是将x映射到更高的维度，它有一个以后我们经常会见到的专有称号**”核函数“**。 

> 比如我们看到的特征有2个：
> x1,x2,组成最先见到的线性函数可以是:<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfiye9yyz1j307y0220sl.jpg" alt="image-20200606224751560" style="zoom:50%;" />
> 但也许这两个特征并不能很好地描述数据，于是我们进行维度的转化，变成了:$$w_1x_1+w_2x_2+w_3x_1x_2+w_4x_1^2+w_5x_2^2$$.
> 于是我们多了三个特征。而这个就是笼统地描述x的映射的。 
> 最简单直接的就是：<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfiyel7s1ej305601k3yb.jpg" alt="image-20200606224809781" style="zoom:50%;" />



以上就是线性可分支持向量机的模型表达式。我们要去求出这样一个模型，或者说这样一个超平面y(x),它能够最优地分离两个集合。 

**其实也就是我们要去求一组参数（w,b),使其构建的超平面函数能够最优地分离两个集合。**

如下就是一个最优超平面： 

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jc7zuhj30oi0f9q93.jpg" alt="image_1b1tqf5qo1s6r1hhq14ct1dfu5d12a.png-231.8kB" style="zoom:67%;" />

又比如说这样： 

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jd1n90j30fi0eg75x.jpg" alt="image_1b1tqguiaj4p1i7m1u9j10t11mds2n.png-69.3kB" style="zoom: 67%;" />

阴影部分是一个“过渡带”，“过渡带”的边界是集合中离超平面最近的样本点落在的地方。



## 3 SVM的计算过程与算法步骤

### 3.1 推导目标函数

我们知道了支持向量机是个什么东西了。现在我们要去寻找这个支持向量机，也就是寻找一个最优的超平面。

于是我们要建立一个目标函数。那么如何建立呢？

再来看一下我们的超平面表达式： $$y(x)=w^T\Phi(x)+b$$

为了方便我们让：$$\Phi(x)=x$$

则在样本空间中，划分超平面可通过如下线性方程来描述：$$w^Tx+b=0$$

- 我们知道<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jknd2kj308o01g0sl.jpg" alt="image-20190814214644095" style="zoom:50%;" />为法向量，决定了超平面的方向；

- b为位移项，决定了超平面和原点之间的距离。

- 显然，划分超平面可被法向量w和位移b确定，我们把其记为（w,b）.


样本空间中任意点x到超平面（w,b）的距离可写成

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jlfgvyj30d802qq2v.jpg" alt="image-20190814141310502" style="zoom: 67%;" />

假设超平面（w, b）能将训练样本正确分类，即对于<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfiykqm139j307i01mt8k.jpg" alt="image-20200606225110280" style="zoom:50%;" />，

- 若y<sub>i</sub>=+1，则有<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfiylmm27ej30ek03gmx7.jpg" alt="image-20200606225455732" style="zoom: 25%;" />;
- 若y<sub>i</sub>=-1，则有<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfiymqdxp7j30ee038t8q.jpg" alt="image-20200606225559186" style="zoom:25%;" />;

令

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jodtsyj30i4034mxb.jpg" alt="image-20190814141642787" style="zoom: 50%;" />



如图所示，距离超平面最近的几个训练样本点使上式等号成立，他们被称为“支持向量"，

两个异类支持向量到超平面的距离之和为:<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jos3ccj302001w3y9.jpg" alt="img" style="zoom:67%;" />；

它被称为“”间隔“”。

![image-20190814141836897](https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jp7483j30sg0bgabn.jpg)

欲找到具有最大间隔的划分超平面，也就是要找到能满足下式中约束的参数w和b，使得 r 最大。

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jodtsyj30i4034mxb.jpg" alt="image-20190814141642787" style="zoom: 50%;" />

即：

![image-20200608172432236](https://tva1.sinaimg.cn/large/007S8ZIlly1gfl0ai298xj31bo07kjte.jpg)

显然，为了最大化间隔，仅需要最大化<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfk2jigvqzj301d0140iv.jpg" alt="img" style="zoom: 67%;" />，这等价于最小化<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfk2jnzh5mj30170140j8.jpg" alt="img" style="zoom:67%;" />。于是上式可以重写为：

<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfl0a03qeqj30sa064jta.jpg" alt="image-20200608172400286" style="zoom:50%;" />

这就是支持向量机的基本型。

> 拓展：什么是 ||w||?
>
> 链接：[8.10拓展阅读：向量与矩阵的范数.md](https://github.com/sherwinNG/budao_AI/blob/main/05-机器学习算法/08-支持向量机/8.10拓展阅读：向量与矩阵的范数.md)



### 3.2 目标函数的求解

到这一步，终于把目标函数给建立起来了。

那么下一步自然是去求目标函数的最优值.

因为目标函数带有一个约束条件，所以**我们可以用拉格朗日乘子法求解**。

#### 3.2.1 朗格朗日乘子法

啥是拉格朗日乘子法呢？

拉格朗日乘子法 (Lagrange multipliers)是**一种寻找多元函数在一组约束下的极值的方法**.

通过引入拉格朗日乘子，可将有 d 个变量与 k 个约束条件的最优化问题转化为具有 d + k 个变量的无约束优化问题求解。

[8.11拓展阅读：朗格朗日乘子法举例.md](https://github.com/sherwinNG/budao_AI/blob/main/05-机器学习算法/08-支持向量机/8.11拓展阅读：朗格朗日乘子法举例.md)

----

经过朗格朗日乘子法，我们可以把目标函数转换为： 

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jusjlpj30hh01qt8o.jpg" alt="image_1b1tslnd0jn516gai3end5rmj5e.png-8.9kB" style="zoom:67%;" />

然后我们令：

<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh84kfzm28j30u20360tb.jpg" alt="image-20200729204050032" style="zoom: 33%;" />

容易验证，当某个约束条件不满足时，例如<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh84p5hkk7j306g01i0sl.jpg" alt="image-20200729204521998" style="zoom: 50%;" />，那么显然有 θ(w) = ∞ （只要令 α<sub>i</sub> = ∞ 即可）。而当所有约束条件都满足时，则有 <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh84qkhvsmj305s01k3yc.jpg" alt="image-20200729204643886" style="zoom:50%;" /> ，亦即最初要 最小化的量。

因此，在要求约束条件得到满足的情况下最小化<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh84rjnrrkj302k01ma9u.jpg" alt="image-20200729204739329" style="zoom:50%;" /> ，实际上等价于直接最小化 θ(w)（当然， 这里也有约束条件， 就是 α <sub>i</sub> ≥ 0, i = 1, …, n），因为如果约束条件没有得 到满足， θ(w) 会等于无穷大，自然不会是我们所要求的最小值。

具体写出来，目标函数变成了：

<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh84ve8vxdj30l002k3yl.jpg" alt="image-20200729205121669" style="zoom:50%;" />

这里用 p* 表示这个问题的最优值，且和最初的问题是等价的。如果直接求解，那么一上来便得面对 w 和 b 两个参数，而 α <sub>i</sub> 又是不等式约束，这个求解过程不好做。

此时，我们可以借助**对偶问题**进行求解。



#### 3.2.2 对偶问题

因为我们在上面求解的过程中，直接求解 w 和 b 两个参数不方便，所以想办法转换为对偶问题。

我们要将其转换为**对偶问题**，变成极大极小值问题：

<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh84zcyti9j30jw02ot8p.jpg" alt="image-20200729205510831" style="zoom:50%;" />

> 参考资料：
> [https://wenku.baidu.com/view/7bf945361b37f111f18583d049649b6649d70975.html](https://wenku.baidu.com/view/7bf945361b37f111f18583d049649b6649d70975.html)



如何获取对偶函数？ 

- 首先我们对原目标函数的w和b分别求导： 
    - 原目标函数：<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jy516cj30kb01yjrd.jpg" alt="image_1b1vjnneag2c1o0esh21520kn69.png-9.6kB" style="zoom:50%;" />
    - 对w求偏导： <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh81vww1hfj30tq040wgd.jpg" alt="image-20200729190803512" style="zoom: 25%;" />
    - 对b求偏导： <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh81ydqk2cj30so04amym.jpg" alt="image-20200729191025851" style="zoom:25%;" />



- 然后将以上w和b的求导函数重新代入原目标函数的w和b中，得到的就是原函数的对偶函数： 

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3k0353jj30k807wjs8.jpg" alt="image_1b1vk20iq3vc14ld17p51a1v1tq72h.png-40.6kB" style="zoom: 67%;" />



- 这个对偶函数其实求的是：<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gb3j3r6wddj309q020aao.jpg" alt="image-20200121011355571" style="zoom: 33%;" />中的$$minL(w,b)$$部分（因为对w,b求了偏导）。

- 于是现在要求的是这个函数的极大值max(a),写成公式就是： 

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gb3j4yqvecj30q4048juj.jpg" alt="image-20200121011505169" style="zoom: 33%;" />



- 好了，现在我们只需要对上式求出极大值α，然后将α代入w求偏导的那个公式：

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gbk6jc5w8bj30ag03wdgq.jpg" alt="image-20200204105305071" style="zoom:50%;" />

- 从而求出w.

- 将w代入超平面的表达式，计算b值；

- 现在的w,b就是我们要寻找的最优超平面的参数。


####  3.2.3 整体流程确定

我们用数学表达式来说明上面的过程： 

- 1）首先是求<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3k63li3j306q015glg.jpg" alt="image_1b1vl4efr1in9ie51i3oji91k443q.png-4.5kB" style="zoom: 50%;" />的极大值。即：

![image-20200204111208245](https://tva1.sinaimg.cn/large/006tNbRwly1gbk72hel9hj317a06o40i.jpg)

> 注意有两个约束条件。

- 对目标函数添加负号，转换成求极小值： 

![image-20200204111226729](https://tva1.sinaimg.cn/large/006tNbRwly1gbk72t1f47j316c06wac2.jpg)

- 2）计算上面式子的极值求出 α*;

- 3）α* 代入，计算w,b 

<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh81mk6izrj31g008a0wu.jpg" alt="image-20200729185904285" style="zoom: 33%;" />

- 4）求得超平面： 

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3kcrcq2j30k7013a9x.jpg" alt="image_1b1vld5fhblkn2j1c7h1nk1pc66e.png-5.3kB" style="zoom: 50%;" />

- 5）求得分类决策函数：

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3kd9p66j30k801d748.jpg" alt="image_1b1vldujb18mb1g5ch8uoikios6r.png-8.2kB" style="zoom: 50%;" />



## 4 举例

给定3个数据点：正例点x1=(3,3),x2=(4,3),负例点x3=(1,1),求线性可分支持向量机。 
三个点画出来：

![image_1b1vpnodb1b799lo8l2e23122h78.png-10.7kB](https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3kdmc0kj307v03zmx6.jpg)

1) 首先确定目标函数

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3keatddj30vw0d4ae7.jpg" alt="image-20190813200353530" style="zoom: 25%;" />

2) 求得目标函数的极值

- 原式：<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gbk5fi9gt9j30ey03smye.jpg" alt="image-20200204101525358" style="zoom: 50%;" />
- 把数据代入：<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gbk5gh3uv7j310g04cjtu.jpg" alt="image-20200204101623739" style="zoom: 50%;" />
- 由于：<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gbk5hdbi30j306o02eq32.jpg" alt="image-20200204101715362" style="zoom:50%;" />
- 化简可得：<img src="https://tva1.sinaimg.cn/large/006tNbRwly1gbk5hrvsyyj30hk036q40.jpg" alt="image-20200204101738823" style="zoom:50%;" />

- 对α1,α2 求偏导并令其为0，易知s(α1， α2)在点（1.5， -1）处取极值。
- 而该点不满足条件α2 >= 0,所以，最小值在边界上达到。
    - 当α1=0 时，最小值$$s(0,\frac{2}{13})=-\frac{2}{13}=-0.1538$$
    - 当α2=0 时，最小值$$s(\frac{1}{4},0)=-\frac{1}{4}=-0.25$$

- 于是，s(α1， α2)在α1=1/4 , α2=0时达到最小，此时：$$\alpha_3 = \alpha_1+\alpha_2 = \frac{1}{4}$$

3) 将求得的极值代入从而求得最优参数w,b 

- <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfjxfcb6uyj305y0243yb.jpg" alt="image-20200607185943671" style="zoom:50%;" />对应的点x<sub>1</sub>, x<sub>3</sub>就是支持向量机 

- 代入公式： 

    - 将α结果代入求解：

    - <img src="https://tva1.sinaimg.cn/large/006tNbRwly1gbk6l8wqjvj30ag03wdgq.jpg" alt="image-20200204105534190" style="zoom:50%;" />

    - ![image-20200729190053487](https://tva1.sinaimg.cn/large/007S8ZIlly1gh81og4uvpj31pq0847al.jpg)

        > 选择α的一个支持向量的正分量α<sub>j</sub>>0进行计算
    
    - 平面方程为：$$0.5x_1+0.5x_2-2=0$$



4) 因此得到分离超平面为: $$0.5x_1+0.5x_2-2=0$$

5) 得到分离决策函数为：$$f(x)=sign(0.5x_1+0.5x_2-2)$$



> ps:参考的另一种计算方式：
> [https://blog.csdn.net/zhizhjiaodelaoshu/article/details/97112073](https://blog.csdn.net/zhizhjiaodelaoshu/article/details/97112073)





------

## 3 小结

- SVM中目标函数

    - <img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3jt2u1bj308x02xa9x.jpg" alt="img" style="zoom:67%;" />

- SVM中目标函数的求解过程

    - 1）首先是求<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3k63li3j306q015glg.jpg" alt="image_1b1vl4efr1in9ie51i3oji91k443q.png-4.5kB" style="zoom: 50%;" />的极大值。即：

    ![image-20200204111208245](https://tva1.sinaimg.cn/large/006tNbRwly1gbk72hel9hj317a06o40i.jpg)

    > 注意有两个约束条件。

    - 对目标函数添加符号，转换成求极小值： 

    ![image-20200204111226729](https://tva1.sinaimg.cn/large/006tNbRwly1gbk72t1f47j316c06wac2.jpg)

    - 2）计算上面式子的极值求出α*;

    - 3）将α*代入，计算w,b

        <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh81lh2co7j31g008a0wu.jpg" alt="image-20200729185759701" style="zoom: 33%;" /> 

    - 4）求得超平面： 

    <img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3kcrcq2j30k7013a9x.jpg" alt="image_1b1vld5fhblkn2j1c7h1nk1pc66e.png-5.3kB" style="zoom: 50%;" />

    - 5）求得分类决策函数：

    <img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaf3kd9p66j30k801d748.jpg" alt="image_1b1vldujb18mb1g5ch8uoikios6r.png-8.2kB" style="zoom: 50%;" />