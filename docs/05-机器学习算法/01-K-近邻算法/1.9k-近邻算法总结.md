# 1.9 KNN算法总结

## 1 k近邻算法优缺点汇总

- **优点：**
    - **简单有效**
    - **重新训练的代价低**
    - **适合类域交叉样本**
        - **KNN方法主要靠周围有限的邻近的样本**,而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合。
    - **适合样本容量比较大的类域自动分类**
        - 该算法比较**适用于样本容量比较大的类域的自动分类**，而那些**样本容量较小的类域采用这种算法比较容易产生误分**。 
    
    ```
    样本量、样本个数与样本容量的关系举例
    
    一个箱子最多能放50个苹果（样本），从中取样30个。
    在这里，苹果是样本，箱子最多能放的个数（即苹果的总数）50是这个样本的样本（容）量，而所抽取的样本个数30则是样本量。 
    ```
    
    

---

- **缺点：**
    - **惰性学习**
        - KNN算法是懒散学习方法（lazy learning,基本上不学习），一些积极学习的算法要快很多

    - **类别评分不是规格化**
        - 不像一些通过概率评分的分类
    - **输出可解释性不强**
        - 例如决策树的输出可解释性就较强
    - **对不均衡的样本不擅长**
        - 当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。该算法只计算“最近的”邻居样本，某一类的样本数量很大，那么或者这类样本并不接近目标样本，或者这类样本很靠近目标样本。无论怎样，数量并不能影响运行结果。可以采用权值的方法（和该样本距离小的邻居权值大）来改进。
    - **计算量较大**
        - 目前常用的解决方法是事先对已知样本点进行剪辑，事先去除对分类作用不大的样本。

