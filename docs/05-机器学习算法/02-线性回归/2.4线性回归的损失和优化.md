# 2.4 线性回归的损失和优化

## 学习目标

- 知道线性回归中损失函数
- 知道使用正规方程对损失函数优化的过程
- 知道使用梯度下降法对损失函数优化的过程

------



假设刚才的房子例子，真实的数据之间存在这样的关系：

```python
真实关系：
真实房子价格 = 0.02×中心区域的距离 + 0.04×城市一氧化氮浓度 + (-0.12×自住房平均房价) + 0.254×城镇犯罪率
```

那么现在呢，我们随意指定一个关系（猜测）

```python
随机指定关系：
预测房子价格 = 0.25×中心区域的距离 + 0.14×城市一氧化氮浓度 + 0.42×自住房平均房价 + 0.34×城镇犯罪率
```

请问这样的话，会发生什么？真实结果与我们预测的结果之间是不是存在一定的误差呢？类似这样样子

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u18alruj30ps0jcmzc.jpg" alt="image-20190221093806586" style="zoom:50%;" />

既然存在这个误差，那我们就将这个误差给衡量出来

## 1 损失函数

总损失定义为：

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u192gzvj30v8066jrl.jpg" alt="çº¿æ§åå½æå¤±å½æ°" style="zoom:50%;" />

- y<sub>i</sub>为第i个训练样本的真实值
- h(x<sub>i</sub>)为第i个训练样本特征值组合预测函数
- 又称最小二乘法

如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！

## 2 优化算法

**如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）**

- 线性回归经常使用的两种优化算法
    - 正规方程
    - 梯度下降法

### 2.1 正规方程

#### 2.1.1 什么是正规方程

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u19hxgmj30bc01qmx4.jpg" alt="æ­£è§æ¹ç¨" style="zoom:50%;" />

> 理解：X为特征值矩阵，y为目标值矩阵。直接求到最好的结果
>
> 缺点：当特征过多过复杂时，求解速度太慢并且得不到结果



![image-20190221094805620](https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1cxsodj31wl0u0wk0.jpg)

#### 2.1.2 正规方程求解举例

以下表示数据为例：

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1dp9vxj30pm0c4die.jpg" alt="image-20190221100240178" style="zoom:67%;" />

即：

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1g7gw1j30oo070t9m.jpg" alt="image-20190221100305355" style="zoom:50%;" />

运用正规方程方法求解参数：

![image-20190709103604510](https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1pztjcj31hw08g3zz.jpg)



#### 2.1.3 正规方程的推导

- **推导方式：**

把该损失函数转换成矩阵写法：

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1t8lt6j30tg08udg6.jpg" style="zoom:50%;" />

其中y是真实值矩阵，X是特征值矩阵，w是权重矩阵

把损失函数分开书写：
$$
(Xw-y)^2=(Xw-y)^T(Xw-y)
$$
对展开上式进行求导：

<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfasuui5wlj31dc06sdgx.jpg" alt="image-20190709113507602" style="zoom: 100%;" />

需要求得求导函数的极小值，即上式求导结果为0，经过化解，得结果为：
$$
X^TXw=X^Ty
$$
经过化解为：
$$
w=(X^TX)^{-1}X^Ty
$$

------

> 补充：需要用到的矩阵求导公式：

$$
\frac{dx^TA}{dx}=A
$$

$$
\frac{dAx}{dx}=A^T
$$

![image-20190915203039165](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjcttc3nhj312k03cq2u.jpg)



### 2.2  梯度下降

#### 2.2.1 什么是梯度下降

梯度下降法(Gradient Descent)的基本思想可以类比为一个下山的过程。

假设这样一个场景：

一个人**被困在山上，需要从山上下来**(i.e. 找到山的最低点，也就是山谷)。但此时山上的浓雾很大，导致可视度很低。

因此，下山的路径就无法确定，他必须利用自己周围的信息去找到下山的路径。这个时候，他就可以利用梯度下降算法来帮助自己下山。

具体来说就是，以他当前的所处的位置为基准，**寻找这个位置最陡峭的地方，然后朝着山的高度下降的地方走**，（同理，如果我们的目标是上山，也就是爬到山顶，那么此时应该是朝着最陡峭的方向往上走）。然后每走一段距离，都反复采用同一个方法，最后就能成功的抵达山谷。

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1vatkkj30v80gyn1o.jpg" alt="image-20190221112607972" style="zoom: 33%;" />

梯度下降的基本过程就和下山的场景很类似。

首先，我们有一个**可微分的函数**。这个函数就代表着一座山。

我们的目标就是找到**这个函数的最小值**，也就是山底。

根据之前的场景假设，最快的下山的方式就是找到当前位置最陡峭的方向，然后沿着此方向向下走，对应到函数中，就是**找到给定点的梯度** ，然后朝着梯度相反的方向，就能让函数值下降的最快！因为梯度的方向就是函数值变化最快的方向。
 所以，我们重复利用这个方法，反复求取梯度，最后就能到达局部的最小值，这就类似于我们下山的过程。而求取梯度就确定了最陡峭的方向，也就是场景中测量方向的手段。



#### 2.2.2 梯度的概念

梯度是微积分中一个很重要的概念

- **在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；**

- **在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；**

    - > 在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。

这也就说明了为什么我们需要千方百计的求取梯度！我们需要到达山底，就需要在每一步观测到此时最陡峭的地方，梯度就恰巧告诉了我们这个方向。梯度的方向是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向，这正是我们所需要的。所以我们只要沿着梯度的反方向一直走，就能走到局部的最低点！

#### 2.2.3 梯度下降举例

- **1. 单变量函数的梯度下降** 

我们假设有一个单变量的函数 :J(θ) = θ<sup>2</sup>

函数的微分:J<sup>、</sup>(θ) = 2θ

初始化，起点为： θ<sup>0</sup> = 1

学习率：α = 0.4

我们开始进行梯度下降的迭代计算过程:

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1w5246j30to0gomzq.jpg" alt="image-20190221102707528" style="zoom:50%;" />

如图，经过四次的运算，也就是走了四步，基本就抵达了函数的最低点，也就是山底

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1x5wjfj30je0imdh3.jpg" alt="image-20190221102725918" style="zoom:50%;" />

- **2.多变量函数的梯度下降** 

我们假设有一个目标函数 ：:J(θ) = θ<sub>1</sub><sup>2</sup> + θ<sub>2</sub><sup>2</sup>

现在要通过梯度下降法计算这个函数的最小值。我们通过观察就能发现最小值其实就是 (0，0)点。但是接下
来，我们会从梯度下降算法开始一步步计算到这个最小值!
我们假设初始的起点为: θ<sup>0</sup> = (1, 3)

初始的学习率为:α = 0.1

函数的梯度为:▽:J(θ) =< 2θ<sub>1</sub>  ,2θ<sub>2</sub>>

进行多次迭代:

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u1yd4swj30u00mqq9h.jpg" alt="image-20190221103158740" style="zoom: 50%;" />

我们发现，已经基本靠近函数的最小值点

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2a7jmfj30ti0gwdjy.jpg" alt="image-20190221103220033" style="zoom:50%;" />

#### 2.2.4 梯度下降公式

![image-20190709161202497](https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2b46xjj30z603wq2z.jpg)

- **1) α是什么含义？**

 α在梯度下降算法中被称作为**学习率**或者**步长**，意味着我们可以通过α来控制每一步走的距离，以保证不要步子跨的太大扯着蛋，哈哈，其实就是不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点！

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2ddvhbj31280i2jxg.jpg" alt="image-20190221113408141" style="zoom:50%;" />

- **2) 为什么梯度要乘以一个负号**？

梯度前加一个负号，就意味着朝着梯度相反的方向前进！我们在前文提到，梯度的方向实际就是函数在此点上升最快的方向！而我们需要朝着下降最快的方向走，自然就是负的梯度的方向，所以此处需要加上负号

我们通过两个图更好理解梯度下降的过程

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2egmojj30ym0motb1.jpg" alt="ååéçæ¢¯åº¦ä¸é" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2gwld0j31120qgtvj.jpg" alt="å¤åéçæ¢¯åº¦ä¸é" style="zoom:50%;" />

**所以有了梯度下降这样一个优化算法，回归就有了"自动学习"的能力**

-  **优化动态图演示**

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1ga8u2pniukg30xc0d2e81.gif" alt="image-20190220211910033" style="zoom: 50%;" />



## 3 梯度下降和正规方程的对比

### 3.1 两种方法对比

| 梯度下降             | 正规方程                        |
| -------------------- | ------------------------------- |
| 需要选择学习率       | 不需要                          |
| 需要迭代求解         | 一次运算得出                    |
| 特征数量较大可以使用 | 需要计算方程，时间复杂度高O(n3) |

经过前面的介绍，我们发现最小二乘法适用简洁高效，比梯度下降这样的迭代法似乎方便很多。但是这里我们就聊聊最小二乘法的局限性。

- 首先，最小二乘法需要计算X<sup>T</sup>X的逆矩阵，**有可能它的逆矩阵不存在**，这样就没有办法直接用最小二乘法了。
    - 此时就需要使用梯度下降法。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让X<sup>T</sup>X的行列式不为0，然后继续使用最小二乘法。
- 第二，当样本特征n非常的大的时候，计算X<sup>T</sup>X的逆矩阵是一个非常耗时的工作（n x n的矩阵求逆），甚至不可行。
    - 此时以梯度下降为代表的迭代法仍然可以使用。
    - 那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。
- 第三，如果拟合函数不是线性的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。
- 第四，以下特殊情况，。
    - 当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。
    - 当样本量m等于特征数n的时候，用方程组求解就可以了。
    - 当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了。



### 3.2 算法选择依据

- 小规模数据：
    - 正规方程：**LinearRegression(不能解决拟合问题)**
    - 岭回归
- 大规模数据：
    - 梯度下降法：**SGDRegressor**



经过前面介绍，我们发现在真正的开发中，我们使用梯度下降法偏多（深度学习中更加明显），下一节中我们会进一步介绍梯度下降法的一些原理。



## 4 小结

- 损失函数【知道】
    - 最小二乘法
- 线性回归优化方法【知道】
    - 正规方程
    - 梯度下降法
- 正规方程 -- 一蹴而就【知道】
    - 利用矩阵的逆,转置进行一步求解
    - 只是适合样本和特征比较少的情况
- 梯度下降法 — 循序渐进【知道】
    - 梯度的概念
        - 单变量 -- 切线
        - 多变量 -- 向量
    - 梯度下降法中关注的两个参数
        - α  -- 就是步长
            - 步长太小 -- 下山太慢
            - 步长太大 -- 容易跳过极小值点(*****)
        - 为什么梯度要加一个负号
            - 梯度方向是上升最快方向,负号就是下降最快方向
- 梯度下降法和正规方程选择依据【知道】
    - 小规模数据：
        - 正规方程：**LinearRegression(不能解决拟合问题)**
        - 岭回归
    - 大规模数据：
        - 梯度下降法：**SGDRegressor**