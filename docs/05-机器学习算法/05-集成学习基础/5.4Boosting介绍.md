# 5.4 Boosting

## 学习目标

- 知道boosting集成原理和实现过程
- 知道bagging和boosting集成的区别
- 知道AdaBoost集成原理

------



## 1 什么是boosting

![image-20190214160534929](https://tva1.sinaimg.cn/large/006y8mN6ly1g8qjib3rk8j30vm08iwpw.jpg)

**随着学习的积累从弱到强**

**简而言之：每新加入一个弱学习器，整体能力就会得到提升**

代表算法：Adaboost，GBDT，XGBoost，LightGBM



## 2 实现过程：

**1.训练第一个学习器**

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaq2a4z9a4j31bk0msjzg.jpg" alt="image-20200109093930261" style="zoom: 33%;" />



**2.调整数据分布**

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaq2awcd8cj31de0p0qeq.jpg" alt="image-20200109094017202" style="zoom:33%;" />



**3.训练第二个学习器**

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaq2bg2mndj31ay0nm7ep.jpg" alt="image-20200109094048990" style="zoom:33%;" />



**4.再次调整数据分布**

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaq2cxve1jj319e0qwtnq.jpg" alt="image-20200109094214835" style="zoom:33%;" />



**5.依次训练学习器，调整数据分布**

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaq2dtyxonj31b20u04hn.jpg" alt="image-20200109094305835" style="zoom:33%;" />



**6.整体过程实现**

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaq2fa7qmyj316y0u04ah.jpg" alt="image-20200109094429509" style="zoom:33%;" />



## 3 bagging与boosting的区别

- 区别一:数据方面
    - Bagging：对数据进行采样训练；
    - Boosting：根据前一轮学习结果调整数据的重要性。
- 区别二:投票方面
    - Bagging：所有学习器平权投票；
    - Boosting：对学习器进行加权投票。
- 区别三:学习顺序
    - Bagging的学习是并行的，每个学习器没有依赖关系；
    - Boosting学习是串行，学习有先后顺序。
- 区别四:主要作用
    - Bagging主要用于提高泛化性能（解决过拟合，也可以说降低方差）
    - Boosting主要用于提高训练精度 （解决欠拟合，也可以说降低偏差）

![image-20200109094753644](https://tva1.sinaimg.cn/large/006tNbRwgy1gaq2iti992j31aa0gk7c9.jpg)



## 4 AdaBoost介绍

### 4.1 构造过程细节

- 步骤一：初始化训练数据权重相等，训练第一个学习器。

    - > 该假设每个训练样本在基分类器的学习中作用相同，这一假设可以保证第一步能够在原始数据上学习基本分类器H<sub>1</sub>(x)

- 步骤二：AdaBoost反复学习基本分类器，在每一轮m=1,2,...,M   顺次的执行下列操作：

    - （a） 在权值分布为D<sub>t</sub>的训练数据上，**确定基分类器；**

    - （b） 计算该学习器在训练数据中的**错误率**：

        $$\varepsilon _t = P(h_t(x_t)\neq y_t)$$

    - （c） 计算该学习器的**投票权重**：

        $$\alpha _t=\frac{1}{2}ln(\frac{1-\varepsilon _t}{\varepsilon _t})$$

    - （d） 根据投票权重，对**训练数据重新赋权**![image-20191108183556749](https://tva1.sinaimg.cn/large/006y8mN6ly1g8qtd52iq7j30z004ata1.jpg)

        - > 将下一轮学习器的注意力集中在错误数据上

    - 重复执行a到d步，m次；

- 步骤三：对m个学习器进行加权投票

    ![image-20191108171049147](https://tva1.sinaimg.cn/large/006y8mN6ly1g8qqwk2tv0j30xe03ojrz.jpg)

### 4.2 关键点剖析

**如何确认投票权重？**

**如何调整数据分布？**

<img src="https://tva1.sinaimg.cn/large/006tNbRwgy1gaq2g8qop2j318y0u0ngu.jpg" alt="image-20200109094524632" style="zoom:33%;" />



### 4.3 案例介绍

给定下面这张训练数据表所示的数据，假设弱分类器由x<v或x>v产生，其阈值v使该分类器在训练数据集上的分类误差率最低，试用Adaboost算法学习一个强分类器。

![image-20191108172411859](https://tva1.sinaimg.cn/large/006y8mN6ly1g8qrahkrjtj314i05owff.jpg)



问题解答：

**步骤一：初始化训练数据权重相等，训练第一个学习器：**

$$D_1=(w_{11},w_{12},...,w_{110},)$$

$$w_{1i}=0.1, i=1,2,...,10$$

-----

**步骤二：AdaBoost反复学习基本分类器，在每一轮m=1,2,...,M顺次的执行下列操作：**



**当m=1的时候：**

（a）在权值分布为D<sub>1</sub>的训练数据上，阈值v取2.5时分类误差率最低，故基本分类器为:

> 6,7,8被分错

![image-20191108183638434](https://tva1.sinaimg.cn/large/006y8mN6ly1g8qtdupoapj30sm03aaai.jpg)

（b）计算该学习器在训练数据中的**错误率**：$$\varepsilon _1 = P(h_1(x_1)\neq y_1)=0.3$$

（c）计算该学习器的**投票权重**：$$\alpha _1=\frac{1}{2}ln(\frac{1-\varepsilon _1}{\varepsilon _1})=0.4236$$

（d）根据投票权重，对**训练数据重新赋权:**

$$D_2=(w_{21},w_{22},...,w_{210},)$$

根据下公式，计算各个权重值![image-20191108183556749](https://tva1.sinaimg.cn/large/006y8mN6ly1g8qtec4x5kj30z004adg1.jpg)

经计算得，D<sub>2</sub>的值为：

$$D_2=(0.07143,0.07143,0.07143,0.07143,0.07143, 0.07143,0.16667,0.16667,0.16667,0.07143)$$

计算过程：

> ![image-20191108184941435](https://tva1.sinaimg.cn/large/006y8mN6ly1g8qtrfjegjj310u036gmc.jpg)

$$H_1(x)=sign[0.4236h_1(x)]$$

分类器H<sub>1</sub>(x)在训练数据集上有3个误分类点。

------

**当m=2的时候：**

（a）在权值分布为D<sub>2</sub>的训练数据上，阈值v取8.5时分类误差率最低，故基本分类器为:

> 3,4,5被分错

![image-20191108185644858](https://tva1.sinaimg.cn/large/006y8mN6ly1g8qtys7zc1j30tk02y74q.jpg)

（b）计算该学习器在训练数据中的**错误率**：$$\varepsilon _2 = P(h_2(x_2)\neq y_2)=0.2143$$

（c）计算该学习器的**投票权重**：$$\alpha _2=\frac{1}{2}ln(\frac{1-\varepsilon _2}{\varepsilon _2})=0.6496$$

（d）根据投票权重，对**训练数据重新赋权:**

经计算得，D<sub>3</sub>的值为：

$$D_3=(0.0455, 0.0455, 0.0455, 0.1667, 0.1667, 0.1667, 0.1060, 0.1060, 0.1060,0.0455)$$



$$H_2(x)=sign[0.4236h_1(x)+0.6496h_2(x)]$$

分类器H<sub>2</sub>(x)在训练数据集上有3个误分类点。

-----

**当m=3的时候：**

（a）在权值分布为D<sub>3</sub>的训练数据上，阈值v取5.5时分类误差率最低，故基本分类器为:

<img src="https://tva1.sinaimg.cn/large/0082zybply1gc9nko0l8lj30h803m0tl.jpg" alt="image-20200226114205989" style="zoom: 50%;" />


（b）计算该学习器在训练数据中的**错误率**：$$\varepsilon _3 = 0.1820$$

（c）计算该学习器的**投票权重**：$$\alpha _3=0.7514$$

（d）根据投票权重，对**训练数据重新赋权:**

经计算得，D<sub>2</sub>的值为：

$$D_4=(0.125, 0.125, 0.125, 0.102, 0.102, 0.102, 0.065, 0.065, 0.065, 0.125)$$



$$H_3(x)=sign[0.4236h_1(x)+0.6496h_2(x)+0.7514h_3(x)]$$

分类器H<sub>3</sub>(x)在训练数据集上的误分类点个数为0。

-----

**步骤三：对m个学习器进行加权投票,获取最终分类器**

$$H_3(x)=sign[0.4236h_1(x)+0.6496h_2(x)+0.7514h_3(x)]$$



### 4.4 api

- from sklearn.ensemble import AdaBoostClassifier
    - [api链接:<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier>](<https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier>)



------

## 5 小结

- 什么是Boosting 【知道】

    - 随着学习的积累从弱到强
    - 代表算法：Adaboost，GBDT，XGBoost，LightGBM

- bagging和boosting的区别【知道】

    - 区别一:数据方面

        - Bagging：对数据进行采样训练；

        - Boosting：根据前一轮学习结果调整数据的重要性。

    - 区别二:投票方面

        - Bagging：所有学习器平权投票；

        - Boosting：对学习器进行加权投票。

    - 区别三:学习顺序

        - Bagging的学习是并行的，每个学习器没有依赖关系；

        - Boosting学习是串行，学习有先后顺序。

    - 区别四:主要作用

        - Bagging主要用于提高泛化性能（解决过拟合，也可以说降低方差）

        - Boosting主要用于提高训练精度 （解决欠拟合，也可以说降低偏差）

- AdaBoost构造过程【知道】

    - 步骤一：初始化训练数据权重相等，训练第一个学习器;
    - 步骤二：AdaBoost反复学习基本分类器;
    - 步骤三：对m个学习器进行加权投票