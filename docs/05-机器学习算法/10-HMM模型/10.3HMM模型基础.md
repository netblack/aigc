# 10.3 HMM模型基础

## 学习目标

- 了解HMM模型解决的问题的主要特征
- 知道HMM模型的两个重要假设
- 指导HMM观测序列的生成过程
- 知道HMM模型的三个基本问题

------



## 1 什么样的问题需要HMM模型

首先我们来看看什么样的问题解决可以用HMM模型。使用HMM模型时我们的问题一般有这两个特征：

- １）我们的问题是基于序列的，比如时间序列，或者状态序列。
- ２）我们的问题中有两类数据，
    - 一类序列数据是可以观测到的，**即观测序列**；
    - 而另一类数据是不能观察到的，**即隐藏状态序列，简称状态序列**。

有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。

- 比如：我现在给大家写课件，我在键盘上敲出来的一系列字符就是**观测序列**，而我实际想写的一段话就是**隐藏状态序列**，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。


- 再举一个，假如我上课讲课，我发出的一串连续的声音就是**观测序列**，而我实际要表达的一段话就是**隐藏状态序列**，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。


从这些例子中，我们可以发现，HMM模型可以无处不在。但是上面的描述还不精确，下面我们用精确的数学符号来表述我们的HMM模型。



## 2 HMM模型的定义

对于HMM模型，首先我们假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：

- $$Q={q_1,q_2,...,q_N}$$
- $$V={v_1,v_2,...v_M}$$

> 其中，N是可能的隐藏状态数，M是所有的可能的观察状态数。



对于一个长度为T的序列，i是对应的状态序列, O是对应的观察序列，即：

- $$i={i_1,i_2,...,i_T}$$
- $$O={o_1,o_2,...o_T}$$

> 其中，任意一个隐藏状态<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmg3x5h4uj3046022wea.jpg" alt="image-20200609231716339" style="zoom:50%;" />, 任意一个观察状态<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmg4dzvftj303w018a9t.jpg" alt="image-20200609231742610" style="zoom:50%;" />



HMM模型做了两个很重要的假设如下：

**1） 齐次马尔科夫链假设。**

- 即**任意时刻的隐藏状态只依赖于它前一个隐藏状态。**
    - 当然这样假设有点极端，**因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。**
    - 但是这样假设的好处就是模型简单，便于求解。
    - 如果在时刻t的隐藏状态是i<sub>t</sub>=q<sub>i</sub>,在时刻 t+1 的隐藏状态是i<sub>t+1</sub>=q<sub>j</sub>, 则从时刻t到时刻t+1的HMM状态转移概率a<sub>ij</sub>可以表示为：
        - $$a_{ij}=P(i_{t+1}= q_j | i_t=q_i)$$
    - 这样a<sub>ij</sub>可以组成马尔科夫链的状态转移矩阵A:
        - $$A=[a_{ij}]_{N \times N}$$



2） **观测独立性假设。**

- 即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。

    - 如果在时刻t的隐藏状态是i<sub>t</sub>=q<sub>j</sub>, 而对应的观察状态为o<sub>t</sub>=v<sub>k</sub>, 则该时刻观察状态v<sub>k</sub>在隐藏状态q<sub>j</sub>下生成的概率为b<sub>j</sub>(k),满足：

        - $$b_j(k)=P(o_t=v_k|i_t=q_j)$$

    - 这样b<sub>j</sub>(k)可以组成观测状态生成的概率矩阵B:

        - $$B=[b_j(k)]_{N \times M}$$

    - 除此之外，我们需要一组在时刻t=1的隐藏状态概率分布Π :

        - $$ \Pi =[\Pi_i]_N$$

        - > $$\Pi _i=P(i_1=q_i)$$



一个HMM模型，可以由**隐藏状态初始概率分布Π, 状态转移概率矩阵A和观测状态概率矩阵B决定**。

Π ,A决定状态序列，B决定观测序列。

因此，HMM模型可以由一个三元组 λ 表示如下：

- $$ \lambda =(A,B, \Pi )=  (状态序列，观测序列，初始状态概率分布)$$




## 3 一个HMM模型实例

下面我们用一个简单的实例来描述上面抽象出的HMM模型。这是一个盒子与球的模型。

> 案例来源于李航-《统计学习方法》。

假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里球的数量分别是：

| 盒子   | 1    | 2    | 3    |
| ------ | ---- | ---- | ---- |
| 红球数 | 5    | 4    | 7    |
| 白球数 | 5    | 6    | 3    |

按照下面的方法从盒子里抽球，开始的时候，

- 从第一个盒子抽球的概率是0.2，
- 从第二个盒子抽球的概率是0.4，
- 从第三个盒子抽球的概率是0.4。

以这个概率抽一次球后，将球放回。

然后从当前盒子转移到下一个盒子进行抽球。规则是：

- 如果当前抽球的盒子是第一个盒子，则以0.5的概率仍然留在第一个盒子继续抽球，以0.2的概率去第二个盒子抽球，以0.3的概率去第三个盒子抽球。
- 如果当前抽球的盒子是第二个盒子，则以0.5的概率仍然留在第二个盒子继续抽球，以0.3的概率去第一个盒子抽球，以0.2的概率去第三个盒子抽球。
- 如果当前抽球的盒子是第三个盒子，则以0.5的概率仍然留在第三个盒子继续抽球，以0.2的概率去第一个盒子抽球，以0.3的概率去第二个盒子抽球。

---

如此下去，直到重复三次，得到一个球的颜色的观测序列:

-  O={红，白，红}

注意在这个过程中，**观察者只能看到球的颜色序列，却不能看到球是从哪个盒子里取出的。**

那么按照我们前面HMM模型的定义，我们的观察状态集合是:

- V={红，白}，M=2

我们的隐藏状态集合是：

- Q={盒子1，盒子2，盒子3}，N=3

而观察序列和状态序列的长度为3.

初始状态分布Π为：

- $$\Pi =(0.2,0.4,0.4)^T$$

状态转移概率分布A矩阵为：

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1g9sm26sp0lj30cs05caa8.jpg" alt="image-20191211111441386" style="zoom:50%;" />

观测状态概率B矩阵为：

<img src="https://tva1.sinaimg.cn/large/006tNbRwly1g9sm2lvf05j309q0560sr.jpg" alt="image-20191211111505163" style="zoom:50%;" />



## 4 HMM观测序列的生成

从上面的例子，我们也可以抽象出HMM观测序列生成的过程。

- 输入的是HMM的模型 λ=(A,B,Π) ,观测序列的长度T

- 输出是观测序列：<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmgba6t38j308401e0sk.jpg" alt="image-20200609232428037" style="zoom:50%;" />


生成的过程如下：

- 1）根据初始状态概率分布 Π 生成隐藏状态i<sub>1</sub>

- 2) for t from 1 to T
    - a. 按照隐藏状态i<sub>t</sub>的观测状态分布b<sub>it</sub>(k)生成观察状态o<sub>t</sub>
    - b. 按照隐藏状态i<sub>t</sub>的状态转移概率分布ai<sub>t</sub>, i<sub>t+1</sub>产生隐藏状态i<sub>t+1</sub>

所有的o<sub>t</sub>一起形成观测序列<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmgbky7xoj308401ewea.jpg" alt="image-20200609232440292" style="zoom:50%;" />



## 5 HMM模型的三个基本问题

HMM模型一共有三个经典的问题需要解决：

1）评估观察序列概率 —— **前向后向的概率计算**

- 即给定模型 λ=(A,B,Π) 和观测序列<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmgc0tbd0j308401e0sk.jpg" alt="image-20200609232510322" style="zoom:50%;" />，计算在模型 λ下某一个观测序列O出现的概率P(O|λ )。
- 这个问题的求解需要用到前向后向算法，是HMM模型三个问题中最简单的。

2）预测问题，也称为解码问题 ——**维特比（Viterbi）算法**

- 即给定模型λ=(A,B,Π) 和观测序列<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmgc0tbd0j308401e0sk.jpg" alt="image-20200609232510322" style="zoom:50%;" />，求给定观测序列条件下，最可能出现的对应的状态序列。
- 这个问题的求解需要用到基于动态规划的维特比算法，是HMM模型三个问题中复杂度居中的算法。

3）模型参数学习问题 —— **鲍姆-韦尔奇（Baum-Welch）算法**(状态未知) ，这是一个学习问题

- 即给定观测序列<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gfmgc0tbd0j308401e0sk.jpg" alt="image-20200609232510322" style="zoom:50%;" />，估计模型 λ=(A,B,Π) 的参数，使该模型下观测序列的条件概率P(O|λ )最大。
- 这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法，是HMM模型三个问题中最复杂的。

接下来的三节，我们将基于这个三个问题展开讨论。



------

## 3 小结

- 什么样的问题可以用HMM模型解决
    - 基于序列的，比如时间序列；
    - 问题中包含两类数据，一类是可以观测到的观测序列；另一类是不能观察到的隐藏状态序列。
- HMM模型的两个重要假设
    - 其次马尔科夫链假设
    - 观测独立性假设
- HMM模型的三个基本问题
    - 评估观察序列概率—— **前向后向的概率计算**
    - 预测问题，也称为解码问题 ——**维特比（Viterbi）算法**
    - 模型参数学习问题 —— **鲍姆-韦尔奇（Baum-Welch）算法**



