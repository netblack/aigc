# 11.6 lightGBM算法api介绍

## 学习目标

- 了解lightGBM算法api中常用的参数

---



## 1 lightGBM的安装

- windows下：

```python
pip3 install lightgbm
```

- mac下：[安装链接](https://github.com/Microsoft/LightGBM/blob/master/docs/Installation-Guide.rst#macos)



## 2 lightGBM参数介绍

### 2.1 Control Parameters

| Control Parameters   | 含义                                                         | 用法                                                         |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| max_depth            | 树的最大深度                                                 | 当模型过拟合时,可以考虑首先降低 max_depth                    |
| min_data_in_leaf     | 叶子可能具有的最小记录数                                     | 默认20，过拟合时用                                           |
| feature_fraction     | 例如 为0.8时，意味着在每次迭代中随机选择80％的参数来建树     | boosting 为 random forest 时用                               |
| bagging_fraction     | 每次迭代时用的数据比例                                       | 用于加快训练速度和减小过拟合                                 |
| early_stopping_round | 如果一次验证数据的一个度量在最近的early_stopping_round 回合中没有提高，模型将停止训练 | 加速分析，减少过多迭代                                       |
| lambda               | 指定正则化                                                   | 0～1                                                         |
| min_gain_to_split    | 描述分裂的最小 gain                                          | 控制树的有用的分裂                                           |
| max_cat_group        | 在 group 边界上找到分割点                                    | 当类别数量很多时，找分割点很容易过拟合时                     |
| n_estimators         | 最大迭代次数                                                 | 最大迭代数不必设置过大，可以在进行一次迭代后，根据最佳迭代数设置 |

### 2.2 Core Parameters

| Core Parameters | 含义       | 用法                                                         |
| --------------- | ---------- | ------------------------------------------------------------ |
| Task            | 数据的用途 | 选择 train 或者 predict                                      |
| application     | 模型的用途 | 选择 regression: 回归时，<br />binary: 二分类时，<br />multiclass: 多分类时 |
| boosting        | 要用的算法 | gbdt， <br />rf: random forest， <br />dart: Dropouts meet Multiple Additive Regression Trees， <br />goss: Gradient-based One-Side Sampling |
| num_boost_round | 迭代次数   | 通常 100+                                                    |
| learning_rate   | 学习率     | 常用 0.1, 0.001, 0.003…                                      |
| num_leaves      | 叶子数量   | 默认 31                                                      |
| device          |            | cpu 或者 gpu                                                 |
| metric          |            | mae: mean absolute error ， <br />mse: mean squared error ， <br />binary_logloss: loss for binary classification ，<br />multi_logloss: loss for multi classification |

### 2.3 IO parameter

| IO parameter        | 含义                                                         |
| ------------------- | ------------------------------------------------------------ |
| max_bin             | 表示 feature 将存入的 bin 的最大数量                         |
| categorical_feature | 如果 categorical_features = 0,1,2， 则列 0，1，2是 categorical 变量 |
| ignore_column       | 与 categorical_features 类似，只不过不是将特定的列视为categorical，而是完全忽略 |
| save_binary         | 这个参数为 true 时，则数据集被保存为二进制文件，下次读数据时速度会变快 |

## 3 调参建议

| IO parameter       | 含义                                                         |
| ------------------ | :----------------------------------------------------------- |
| `num_leaves`       | 取值应 <= 2<sup>(max\_depth)</sup>， 超过此值会导致过拟合    |
| `min_data_in_leaf` | 将它设置为较大的值可以避免生长太深的树，但可能会导致 underfitting，在大型数据集时就设置为数百或数千 |
| `max_depth`        | 这个也是可以限制树的深度                                     |

下表对应了 Faster Speed ，better accuracy ，over-fitting 三种目的时，可以调的参数

| Faster Speed                                                 | better accuracy                                              | over-fitting                                                 |
| ------------------------------------------------------------ | :----------------------------------------------------------- | :----------------------------------------------------------- |
| 将 `max_bin` 设置小一些                                      | 用较大的 `max_bin`                                           | `max_bin` 小一些                                             |
|                                                              | `num_leaves` 大一些                                          | `num_leaves` 小一些                                          |
| 用 `feature_fraction`来做 `sub-sampling`                     |                                                              | 用 `feature_fraction`                                        |
| 用 `bagging_fraction` <br />和`bagging_freq`【bagging的次数。0表示禁用bagging，非零值表示执行k次bagging】 |                                                              | 设定 `bagging_fraction 和 bagging_freq`                      |
|                                                              | training data 多一些                                         | training data 多一些                                         |
| 用 `save_binary`来加速数据加载                               | 直接用 categorical feature                                   | 用 `gmin_data_in_leaf 和 min_sum_hessian_in_leaf【和xgboost中的min_child_weight类似】` |
| 用 parallel learning                                         | 用 dart<br />【DART利用了深度神经网络中dropout设置的技巧，随机丢弃生成的决策树，然后再从剩下的决策树集中迭代优化提升树】 | 用 `lambda_l1, lambda_l2 ，min_gain_to_split` 做正则化       |
|                                                              | `num_iterations` 大一些，`learning_rate`小一些               | 用 `max_depth` 控制树的深度                                  |

### 
