# 11.2 xgboost算法api介绍

## 学习目标

- 了解xgboost算法api中常用的参数

---



## 1 xgboost的安装：

官网链接：[https://xgboost.readthedocs.io/en/latest/](https://xgboost.readthedocs.io/en/latest/)

```python
pip3 install xgboost
```



## 2 xgboost参数介绍

xgboost虽然被称为kaggle比赛神奇，但是，我们要想训练出不错的模型，必须要给参数传递合适的值。

xgboost中封装了很多参数，主要由三种类型构成：**通用参数（general parameters），Booster 参数（booster parameters）和学习目标参数（task parameters）**

- 通用参数：主要是**宏观函数控制；**
- Booster参数：取决于选择的Booster类型，**用于控制每一步的booster（tree, regressiong）**；
- 学习目标参数：**控制训练目标的表现**。

### 2.1 通用参数（general parameters）

1. **booster** [缺省值=gbtree]

- 决定使用哪个booster，可以是gbtree，gblinear或者dart。 
    - g**btree和dart使用基于树的模型(dart 主要多了 Dropout)，而gblinear 使用线性函数.**

2. **silent** [缺省值=0]

    - 设置为0打印运行信息；设置为1静默模式，不打印

3. **nthread** [缺省值=设置为最大可能的线程数]

    - 并行运行xgboost的线程数，输入的参数应该<=系统的CPU核心数，若是没有设置算法会检测将其设置为CPU的全部核心数

> 下面的两个参数不需要设置，使用默认的就好了

4. **num_pbuffer** [xgboost自动设置，不需要用户设置]

    - 预测结果缓存大小，通常设置为训练实例的个数。该缓存用于保存最后boosting操作的预测结果。

5. **num_feature** [xgboost自动设置，不需要用户设置]

    - 在boosting中使用特征的维度，设置为特征的最大维度



### 2.2 Booster 参数（booster parameters）

#### 2.2.1 Parameters for Tree Booster

1. **eta** [缺省值=0.3，别名：learning_rate]

    - 更新中减少的步长来防止过拟合。

    - 在每次boosting之后，可以直接获得新的特征权值，这样可以使得boosting更加鲁棒。
    - 范围： [0,1]

2. **gamma** [缺省值=0，别名: min_split_loss]（分裂最小loss）

    - 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。
    - Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。

    - 范围: [0,∞]

3. **max_depth** [缺省值=6]

    - 这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。设置为0代表没有限制
    - 范围: [0,∞]

4. **min_child_weight** [缺省值=1]

    - 决定最小叶子节点样本权重和。XGBoost的这个参数是最小样本权重的和.
    - 当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。.
    - 范围: [0,∞]

5. **subsample** [缺省值=1]

    - 这个参数控制对于每棵树，随机采样的比例。
    - 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 

    - 典型值：0.5-1，0.5代表平均采样，防止过拟合.
    - 范围: (0,1]

6. **colsample_bytree** [缺省值=1]

    - 用来控制每棵随机采样的列数的占比(每一列是一个特征)。 
    - 典型值：0.5-1
    - 范围: (0,1]

7. **colsample_bylevel** [缺省值=1]

    - 用来控制树的每一级的每一次分裂，对列数的采样的占比。
    - 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。
    - 范围: (0,1]

8. **lambda** [缺省值=1，别名: reg_lambda]

    - 权重的L2正则化项(和Ridge regression类似)。 
    - 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数
    - 在减少过拟合上还是可以挖掘出更多用处的。.

9. **alpha** [缺省值=0，别名: reg_alpha]

    - 权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。

10. **scale_pos_weight**[缺省值=1]

    - 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。通常可以将其设置为负
    - 样本的数目与正样本数目的比值。

#### 2.2.2 Parameters for Linear Booster

> linear booster一般很少用到。

1. **lambda** [缺省值=0，别称: reg_lambda]

    - L2正则化惩罚系数，增加该值会使得模型更加保守。

2. **alpha** [缺省值=0，别称: reg_alpha]

    - L1正则化惩罚系数，增加该值会使得模型更加保守。

3. **lambda_bias** [缺省值=0，别称: reg_lambda_bias]

    - 偏置上的L2正则化(没有在L1上加偏置，因为并不重要)



### 2.3 学习目标参数（task parameters）

1. **objective** [缺省值=reg:linear]

    1. “**reg:linear**” – 线性回归
    2. **“reg:logistic**” – 逻辑回归
    3. “**binary:logistic**” – 二分类逻辑回归，输出为概率
    4. “**multi:softmax**” – 使用softmax的多分类器，返回预测的类别(不是概率)。在这种情况下，你还需要多设一个参数：num_class(类别数目)
    5. “**multi:softprob**” – 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。

2. **eval_metric** [缺省值=通过目标函数选择]

    可供选择的如下所示：

    1. “**rmse**”: 均方根误差
    2. “**mae**”: 平均绝对值误差
    3. “**logloss**”: 负对数似然函数值
    4. “**error”**: 二分类错误率。
        - 其值通过错误分类数目与全部分类数目比值得到。对于预测，预测值大于0.5被认为是正类，其它归为负类。
    5. “**error@t**”: 不同的划分阈值可以通过 ‘t’进行设置
    6. “**merror**”: 多分类错误率，计算公式为(wrong cases)/(all cases)
    7. “**mlogloss**”: 多分类log损失
    8. “**auc**”: 曲线下的面积

3. seed [缺省值=0]

    - 随机数的种子 
    - 设置它可以复现随机数据的结果，也可以用于调整参数
    