# 11.5 lightGBM

## 学习目标

- 了解lightGBM演进过程
- 知道什么是lightGBM
- 知道lightGBM原理

---



## 1 写在介绍lightGBM之前

### 1.1 lightGBM演进过程

![image-20191202130509524](https://tva1.sinaimg.cn/large/006tNbRwly1g9iaoci9pdj31b606mq3x.jpg)

### 1.2 AdaBoost算法

AdaBoost是一种提升树的方法，和三个臭皮匠，赛过诸葛亮的道理一样。

AdaBoost两个问题：

- (1) 如何改变训练数据的权重或概率分布
    - 提高前一轮被弱分类器错误分类的样本的权重，降低前一轮被分对的权重
- (2) 如何将弱分类器组合成一个强分类器，亦即，每个分类器，前面的权重如何设置
    - 采取”多数表决”的方法.加大分类错误率小的弱分类器的权重，使其作用较大，而减小分类错误率大的弱分类器的权重，使其在表决中起较小的作用。

### 1.3 GBDT算法以及优缺点

GBDT和AdaBosst很类似，但是又有所不同。

- GBDT和其它Boosting算法一样，通过**将表现一般的几个模型（通常是深度固定的决策树）组合在一起来集成一个表现较好的模型。**


- AdaBoost是**通过提升错分数据点的权重来定位模型的不足**， Gradient Boosting通过负梯度来识别问题，通过计算负梯度来改进模型，即通过反复地选择一个指向负梯度方向的函数，该算法可被看做在函数空间里对目标函数进行优化。


因此可以说 。

$$GradientBoosting=GradientDescent+Boosting$$	

---

缺点：

GBDT ->预排序方法(pre-sorted)

- (1) **空间消耗大**。
    - 这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如排序后的索引，为了后续快速的计算分割点），这里**需要消耗训练数据两倍的内存**。
- (2) **时间上也有较大的开销。**
    - 在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。
- (3) **对内存(cache)优化不友好。**
    - 在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。
    - 同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。

### 1.4 启发

常用的机器学习算法，例如神经网络等算法，**都可以以mini-batch的方式训练，训练数据的大小不会受到内存限制。**

而GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。

如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。

尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。

LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。



## 2 什么是lightGBM

lightGBM是2017年1月，微软在GItHub上开源的一个新的梯度提升框架。

[github介绍链接](https://github.com/Microsoft/LightGBM)

在开源之后，就被别人冠以“速度惊人”、“支持分布式”、“代码清晰易懂”、“占用内存小”等属性。

LightGBM主打的高效并行训练让其性能超越现有其他boosting工具。在Higgs数据集上的试验表明，LightGBM比XGBoost快将近10倍，内存占用率大约为XGBoost的1/6。

> higgs数据集介绍：这是一个分类问题，用于区分产生希格斯玻色子的信号过程和不产生希格斯玻色子的信号过程。
>
> [数据链接](https://archive.ics.uci.edu/ml/datasets/HIGGS)



## 3 lightGBM原理

**lightGBM 主要基于以下方面优化，提升整体特特性：**

1. 基于Histogram（直方图）的决策树算法
2. Lightgbm 的Histogram（直方图）做差加速
3. 带深度限制的Leaf-wise的叶子生长策略
4. 直接支持类别特征
5. 直接支持高效并行

具体解释见下，分节介绍。

----

### 3.1 基于Histogram(直方图)的决策树算法

直方图算法的基本思想是

- 先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。
- 在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

> Eg：
>
> [0, 0.1) --> 0; 
>
> [0.1,0.3) --> 1;
>
> ...



<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9ermmsq2cj31y60mu7fj.jpg" alt="image-20191129114925562" style="zoom: 50%;" />



使用直方图算法有很多优点。首先，**最明显就是内存消耗的降低，**直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。

![image-20191129115042904](https://tva1.sinaimg.cn/large/006y8mN6gy1g9ernz2t29j321s0fs7ea.jpg)

然后**在计算上的代价也大幅降低**，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data*#feature)优化到O(k*#features)。

当然，Histogram算法并不是完美的。由于特征被离散化后，**找到的并不是很精确的分割点，所以会对结果产生影响。**但在不同的数据集上的结果表明，离散化的分割点对最终的精度影响并不是很大，甚至有时候会更好一点。原因是决策树本来就是弱模型，分割点是不是精确并不是太重要；较粗的分割点也有正则化的效果，可以有效地防止过拟合；即使单棵树的训练误差比精确分割的算法稍大，但在梯度提升（Gradient Boosting）的框架下没有太大的影响。

### 3.2 Lightgbm 的Histogram(直方图)做差加速

一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。

通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。

利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。

![image-20191129115130409](https://tva1.sinaimg.cn/large/006y8mN6gy1g9erosr1c9j32hu0ckte9.jpg)

### 3.3 带深度限制的Leaf-wise的叶子生长策略

**Level-wise**遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。

- 但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9erf2mdu3j31420ecn3b.jpg" alt="image-20191129114209679" style="zoom:50%;" />

**Leaf-wise**则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。

- 因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。
- Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

<img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g9erg9rycsj313s0ekwl1.jpg" alt="image-20191129114318484" style="zoom:50%;" />

### 3.4 直接支持类别特征

实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的0/1特征，降低了空间和时间的效率。

而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。

在Expo数据集上的实验，相比0/1展开的方法，训练速度可以加速8倍，并且精度一致。目前来看，LightGBM是第一个直接支持类别特征的GBDT工具。

> Expo数据集介绍：[数据](http://stat-computing.org/dataexpo/2009/the-data.html)包含1987年10月至2008年4月美国境内所有商业航班的航班到达和离开的详细信息。这是一个庞大的数据集：总共有近1.2亿条记录。主要用于预测航班是否准时。
>
> [数据链接](http://stat-computing.org/dataexpo/2009/)

### 3.5 直接支持高效并行

LightGBM还具有支持高效并行的优点。LightGBM原生支持并行学习，目前支持特征并行和数据并行的两种。

- 特征并行的主要思想是在不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。
- 数据并行则是让不同的机器先在本地构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。

LightGBM针对这两种并行方法都做了优化:

- 在**特征并行**算法中，通过在本地保存全部数据避免对数据切分结果的通信；

    ![image-20191129115433096](https://tva1.sinaimg.cn/large/006y8mN6gy1g9ersljqo9j31ks0u0tcr.jpg)

- 在**数据并行**中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。

    ![image-20191129115550512](https://tva1.sinaimg.cn/large/006y8mN6gy1g9ertbbed5j31ui0mgto6.jpg)

- **基于投票的数据并行(Voting Parallelization)**则进一步优化数据并行中的通信代价，使通信代价变成常数级别。在数据量很大的时候，使用投票并行可以得到非常好的加速效果。

    ![image-20191129115637083](https://tva1.sinaimg.cn/large/006y8mN6gy1g9eru504p0j31rj0u0x3i.jpg)





---

## 4 小结

- lightGBM 演进过程

    ![image-20191202130509524](https://tva1.sinaimg.cn/large/006tNbRwly1g9iaoci9pdj31b606mq3x.jpg)

- lightGBM优势

    - 基于Histogram（直方图）的决策树算法
    - Lightgbm 的Histogram（直方图）做差加速
    - 带深度限制的Leaf-wise的叶子生长策略
    - 直接支持类别特征
    - 直接支持高效并行